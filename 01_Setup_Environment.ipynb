{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "077b8579-9e54-4536-b263-dbc63488577b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Delta Lake Training - Environment Setup\n",
    "\n",
    "This notebook sets up the training environment including:\n",
    "- Catalog: `databricks_training`\n",
    "- Schema: `delta_demo`\n",
    "- Volumes: `raw_data`, `bronze`, `silver`, `gold`\n",
    "- Generates sample retail datasets with realistic data quality issues\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0f9cd25-77f6-4f57-84e3-046b24c40f5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: Create Catalog\n",
    "spark.sql(\"CREATE CATALOG IF NOT EXISTS avish_practice\")\n",
    "spark.sql(\"USE CATALOG avish_practice\")\n",
    "print(\"✓ Catalog 'avish_practice' created/verified\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37d32d8a-bb37-44ac-8653-1502604ddbaa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 2: Create Schema\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS delta_demo\")\n",
    "spark.sql(\"USE SCHEMA delta_demo\")\n",
    "print(\"✓ Schema 'delta_demo' created/verified\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a512e988-cb79-4614-8a5c-c1544ae07526",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 3: Create Volumes\n",
    "volumes = ['raw_data', 'bronze', 'silver', 'gold']\n",
    "\n",
    "for volume in volumes:\n",
    "    spark.sql(f\"CREATE VOLUME IF NOT EXISTS delta_demo.{volume}\")\n",
    "    print(f\"✓ Volume '{volume}' created/verified\")\n",
    "\n",
    "print(\"\\nAll volumes created successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63f4acbc-d5f3-4491-9a64-4b5f7f68d9fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verify volumes\n",
    "spark.sql(\"SHOW VOLUMES IN delta_demo\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7bbcc1fc-20c5-4330-9560-503ec2ce1f13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Generate Sample Retail Datasets\n",
    "\n",
    "We'll create:\n",
    "- **Customers**: Customer master data\n",
    "- **Products**: Product catalog\n",
    "- **Geographies**: Geographic reference data\n",
    "- **Orders**: Transaction fact table (500k+ rows) with partitions\n",
    "\n",
    "Data quality issues introduced:\n",
    "- Date inconsistencies (different formats, invalid dates)\n",
    "- Missing/null values\n",
    "- Incorrect data types\n",
    "- Bad records (negative values, out of range)\n",
    "- Multiple file formats (CSV and Parquet)\n",
    "- Partitioned by date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1c08a84-17d4-4b28-a3fa-bce2e04fe350",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Base path for volumes\n",
    "base_path = \"/Volumes/avish_practice//delta_demo\"\n",
    "\n",
    "print(\"Starting data generation...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "686d13f7-f179-4ce7-8837-3ec4ce15caa3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Untitled"
    }
   },
   "outputs": [],
   "source": [
    "# Generate Geographies Data\n",
    "geographies = {\n",
    "    'geography_id': range(1, 51),\n",
    "    'country': ['USA'] * 30 + ['Canada'] * 10 + ['Mexico'] * 10,\n",
    "    'state': ['CA', 'NY', 'TX', 'FL', 'IL'] * 10,\n",
    "    'city': [f'City_{i}' for i in range(1, 51)],\n",
    "    'postal_code': [f'{random.randint(10000, 99999)}' for _ in range(50)]\n",
    "}\n",
    "\n",
    "df_geographies = pd.DataFrame(geographies)\n",
    "# Introduce some nulls\n",
    "df_geographies.loc[df_geographies.sample(frac=0.1).index, 'postal_code'] = None\n",
    "\n",
    "# Save as CSV\n",
    "geo_path = f\"{base_path}/raw_data/geographies.csv\"\n",
    "df_geographies.to_csv(geo_path, index=False)\n",
    "print(f\"✓ Created geographies.csv with {len(df_geographies)} records\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83b0b140-aa0a-4387-8db7-0aa4e175af30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Generate Products Data\n",
    "product_categories = ['Electronics', 'Clothing', 'Food', 'Books', 'Toys', 'Home', 'Sports', 'Beauty']\n",
    "products = {\n",
    "    'product_id': range(1, 1001),\n",
    "    'product_name': [f'Product_{i}' for i in range(1, 1001)],\n",
    "    'category': [random.choice(product_categories) for _ in range(1000)],\n",
    "    'price': np.round(np.random.uniform(10, 500, 1000), 2),\n",
    "    'cost': np.round(np.random.uniform(5, 400, 1000), 2),\n",
    "    'supplier_id': [random.randint(1, 50) for _ in range(1000)]\n",
    "}\n",
    "\n",
    "df_products = pd.DataFrame(products)\n",
    "# Introduce bad data: negative prices, nulls\n",
    "bad_indices = df_products.sample(frac=0.05).index\n",
    "df_products.loc[bad_indices, 'price'] = -abs(df_products.loc[bad_indices, 'price'])\n",
    "df_products.loc[df_products.sample(frac=0.08).index, 'category'] = None\n",
    "df_products.loc[df_products.sample(frac=0.03).index, 'supplier_id'] = None\n",
    "\n",
    "# Save as Parquet (partitioned)\n",
    "products_path = f\"{base_path}/raw_data/products\"\n",
    "df_products.to_parquet(products_path, index=False, partition_cols=['category'])\n",
    "print(f\"✓ Created products parquet with {len(df_products)} records (partitioned by category)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbea5ec9-e135-4ace-96e9-6ff8f4d9cd68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Generate Customers Data\n",
    "first_names = ['John', 'Jane', 'Mike', 'Sarah', 'David', 'Emily', 'Chris', 'Lisa', 'Tom', 'Amy']\n",
    "last_names = ['Smith', 'Johnson', 'Williams', 'Brown', 'Jones', 'Garcia', 'Miller', 'Davis', 'Rodriguez', 'Martinez']\n",
    "\n",
    "customers = {\n",
    "    'customer_id': range(1, 10001),\n",
    "    'first_name': [random.choice(first_names) for _ in range(10000)],\n",
    "    'last_name': [random.choice(last_names) for _ in range(10000)],\n",
    "    'email': [f'customer_{i}@example.com' for i in range(1, 10001)],\n",
    "    'phone': [f'{random.randint(100, 999)}-{random.randint(100, 999)}-{random.randint(1000, 9999)}' for _ in range(10000)],\n",
    "    'geography_id': [random.randint(1, 50) for _ in range(10000)],\n",
    "    'registration_date': [(datetime(2020, 1, 1) + timedelta(days=random.randint(0, 1460))).strftime('%Y-%m-%d') for _ in range(10000)]\n",
    "}\n",
    "\n",
    "df_customers = pd.DataFrame(customers)\n",
    "# Introduce date inconsistencies and nulls\n",
    "date_inconsistencies = df_customers.sample(frac=0.1).index\n",
    "df_customers.loc[date_inconsistencies[:len(date_inconsistencies)//3], 'registration_date'] = \\\n",
    "    [(datetime(2020, 1, 1) + timedelta(days=random.randint(0, 1460))).strftime('%m/%d/%Y') for _ in range(len(date_inconsistencies)//3)]\n",
    "df_customers.loc[date_inconsistencies[len(date_inconsistencies)//3:2*len(date_inconsistencies)//3], 'registration_date'] = \\\n",
    "    [(datetime(2020, 1, 1) + timedelta(days=random.randint(0, 1460))).strftime('%d-%m-%Y') for _ in range(len(date_inconsistencies)//3)]\n",
    "df_customers.loc[date_inconsistencies[2*len(date_inconsistencies)//3:], 'registration_date'] = 'invalid_date'\n",
    "\n",
    "# Add nulls\n",
    "df_customers.loc[df_customers.sample(frac=0.05).index, 'phone'] = None\n",
    "df_customers.loc[df_customers.sample(frac=0.03).index, 'geography_id'] = None\n",
    "\n",
    "# Save as CSV (multiple files to simulate partitions)\n",
    "customers_path = f\"{base_path}/raw_data/customers\"\n",
    "os.makedirs(customers_path, exist_ok=True)\n",
    "chunk_size = 2500\n",
    "for i in range(0, len(df_customers), chunk_size):\n",
    "    chunk = df_customers.iloc[i:i+chunk_size]\n",
    "    chunk.to_csv(f\"{customers_path}/customers_part_{i//chunk_size + 1}.csv\", index=False)\n",
    "print(f\"✓ Created customers CSV files with {len(df_customers)} records (4 partitions)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45c387e6-96be-40a6-a27b-3e7ef73e5e1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Generate Orders Data (500k+ rows) - Fact Table\n",
    "num_orders = 500000\n",
    "start_date = datetime(2023, 1, 1)\n",
    "end_date = datetime(2024, 12, 31)\n",
    "\n",
    "orders = {\n",
    "    'order_id': range(1, num_orders + 1),\n",
    "    'customer_id': [random.randint(1, 10000) for _ in range(num_orders)],\n",
    "    'product_id': [random.randint(1, 1000) for _ in range(num_orders)],\n",
    "    'order_date': [(start_date + timedelta(days=random.randint(0, (end_date - start_date).days))).strftime('%Y-%m-%d') for _ in range(num_orders)],\n",
    "    'quantity': [random.randint(1, 10) for _ in range(num_orders)],\n",
    "    'unit_price': np.round(np.random.uniform(10, 500, num_orders), 2),\n",
    "    'discount': np.round(np.random.uniform(0, 0.3, num_orders), 2),\n",
    "    'shipping_cost': np.round(np.random.uniform(5, 50, num_orders), 2)\n",
    "}\n",
    "\n",
    "df_orders = pd.DataFrame(orders)\n",
    "\n",
    "# Introduce data quality issues\n",
    "# Date inconsistencies\n",
    "date_issues = df_orders.sample(frac=0.15).index\n",
    "df_orders.loc[date_issues[:len(date_issues)//4], 'order_date'] = \\\n",
    "    [(start_date + timedelta(days=random.randint(0, (end_date - start_date).days))).strftime('%m/%d/%Y') for _ in range(len(date_issues)//4)]\n",
    "df_orders.loc[date_issues[len(date_issues)//4:len(date_issues)//2], 'order_date'] = \\\n",
    "    [(start_date + timedelta(days=random.randint(0, (end_date - start_date).days))).strftime('%d-%m-%Y') for _ in range(len(date_issues)//4)]\n",
    "df_orders.loc[date_issues[len(date_issues)//2:3*len(date_issues)//4], 'order_date'] = \\\n",
    "    [(start_date + timedelta(days=random.randint(0, (end_date - start_date).days))).strftime('%Y%m%d') for _ in range(len(date_issues)//4)]\n",
    "df_orders.loc[date_issues[3*len(date_issues)//4:], 'order_date'] = 'invalid'  # Invalid dates\n",
    "\n",
    "# Bad data: negative quantities, excessive discounts, nulls\n",
    "bad_indices = df_orders.sample(frac=0.02).index\n",
    "df_orders.loc[bad_indices, 'quantity'] = -abs(df_orders.loc[bad_indices, 'quantity'])\n",
    "\n",
    "bad_discounts = df_orders.sample(frac=0.01).index\n",
    "df_orders.loc[bad_discounts, 'discount'] = np.random.uniform(1.0, 2.0, len(bad_discounts))  # Discount > 100%\n",
    "\n",
    "# Nulls\n",
    "df_orders.loc[df_orders.sample(frac=0.04).index, 'discount'] = None\n",
    "df_orders.loc[df_orders.sample(frac=0.02).index, 'shipping_cost'] = None\n",
    "df_orders.loc[df_orders.sample(frac=0.01).index, 'customer_id'] = None\n",
    "\n",
    "# Save as Parquet partitioned by order_date (year-month)\n",
    "df_orders['order_date_parsed'] = pd.to_datetime(df_orders['order_date'], errors='coerce')\n",
    "df_orders['year_month'] = df_orders['order_date_parsed'].dt.to_period('M').astype(str)\n",
    "df_orders = df_orders.drop('order_date_parsed', axis=1)\n",
    "\n",
    "orders_path = f\"{base_path}/raw_data/orders\"\n",
    "os.makedirs(orders_path, exist_ok=True)\n",
    "\n",
    "# Save in chunks by year_month partition\n",
    "for partition in df_orders['year_month'].dropna().unique():\n",
    "    partition_data = df_orders[df_orders['year_month'] == partition].drop('year_month', axis=1)\n",
    "    partition_data.to_parquet(f\"{orders_path}/year_month={partition}\", index=False, partition_cols=None)\n",
    "\n",
    "print(f\"✓ Created orders parquet with {len(df_orders)} records (partitioned by year_month)\")\n",
    "print(f\"  Partitions: {df_orders['year_month'].nunique()} unique year-month combinations\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "012fb86d-b2ba-4926-aa91-9004a7f8a981",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verify all files created\n",
    "print(\"\\n=== Data Generation Summary ===\")\n",
    "print(f\"Geographies: {base_path}/raw_data/geographies.csv\")\n",
    "print(f\"Products: {base_path}/raw_data/products/ (partitioned by category)\")\n",
    "print(f\"Customers: {base_path}/raw_data/customers/ (4 CSV files)\")\n",
    "print(f\"Orders: {base_path}/raw_data/orders/ (partitioned by year_month)\")\n",
    "print(\"\\n✓ Setup complete! Ready for data transformation notebooks.\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_Setup_Environment",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
